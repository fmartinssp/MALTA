{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f18348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library import\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e848cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean imputation function\n",
    "def mean_imputation(missing_data):\n",
    "    complete_data = missing_data.fillna(missing_data.mean()) \n",
    "    return complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imputena import knn\n",
    "\n",
    "#KNN imputation function\n",
    "def knn_imputation(missing_data, k):\n",
    "    \n",
    "    complete_data = knn(missing_data, k=k, inplace=False)\n",
    "    return complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e927a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from missingpy import MissForest\n",
    "\n",
    "#MissForest imputation function\n",
    "def missforest_imputation(missing_data):\n",
    "    columns = missing_data.columns\n",
    "    imputer = MissForest(max_iter=10)\n",
    "    complete_data = imputer.fit_transform(missing_data)\n",
    "    complete_data = pd.DataFrame(complete_data, columns = columns)\n",
    "    return complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imputena import mice\n",
    "\n",
    "#Multiple imputation function\n",
    "def multiple_imputation(missing_data,num_imputations):\n",
    "    columns = missing_data.columns\n",
    "    complete_data = mice(missing_data, num_imputations)\n",
    "    #Temporary database\n",
    "    zero_data = np.zeros(shape=(missing_data.shape[0],missing_data.shape[1])) \n",
    "    df = pd.DataFrame(zero_data, columns = columns)\n",
    "    \n",
    "    #Combining the imputations\n",
    "    for d in complete_data:\n",
    "        for col in d.columns:\n",
    "            df[col]+=d[col]\n",
    "            \n",
    "    complete_data = round(df/5,4)\n",
    "    complete_data = pd.DataFrame(complete_data, columns = columns)\n",
    "    return complete_data            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Kmeans imputation function\n",
    "def kmeans_imputation(missing_data, n_clusters, max_iter):\n",
    "    \"\"\"Perform K-Means clustering on data with missing values.\n",
    "\n",
    "    Args:\n",
    "      missing_data: An [n_samples, n_features] array of data to cluster.\n",
    "      n_clusters: Number of clusters to form.\n",
    "      max_iter: Maximum number of EM iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "      labels: An [n_samples] vector of integer labels.\n",
    "      centroids: An [n_clusters, n_features] array of cluster centroids.\n",
    "      complete_data: Copy of missing_data with the missing values filled in.\n",
    "    \"\"\"\n",
    "\n",
    "    columns = missing_data.columns\n",
    "    # Initialize missing values to their column means\n",
    "    missing = ~np.isfinite(missing_data)\n",
    "    mu = np.nanmean(missing_data, 0, keepdims=1)\n",
    "    complete_data = np.where(missing, mu, missing_data)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        if i > 0:\n",
    "            # initialize KMeans with the previous set of centroids. this is much\n",
    "            # faster and makes it easier to check convergence (since labels\n",
    "            # won't be permuted on every iteration), but might be more prone to\n",
    "            # getting stuck in local minima.\n",
    "            cls = KMeans(n_clusters, init=prev_centroids)\n",
    "        else:\n",
    "            # do multiple random initializations in parallel\n",
    "            cls = KMeans(n_clusters)\n",
    "\n",
    "        # perform clustering on the filled-in data\n",
    "        labels_c = cls.fit_predict(complete_data)\n",
    "        centroids = cls.cluster_centers_\n",
    "\n",
    "        # fill in the missing values based on their cluster centroids\n",
    "        complete_data[missing] = centroids[labels_c][missing]\n",
    "\n",
    "        # when the labels have stopped changing then we have converged\n",
    "        if i > 0 and np.all(labels_c == prev_labels):\n",
    "            break\n",
    "\n",
    "        prev_labels = labels_c\n",
    "        prev_centroids = cls.cluster_centers_\n",
    "\n",
    "    complete_data = pd.DataFrame(complete_data, columns = columns)\n",
    "    return complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imputena import linear_regression\n",
    "\n",
    "#Regression imputation function\n",
    "def regression_imputation(missing_data):\n",
    "    columns = missing_data.columns\n",
    "    complete_data = linear_regression(missing_data, regressions='available')\n",
    "    complete_data = pd.DataFrame(complete_data, columns = columns)\n",
    "    return complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAIN imputation Method\n",
    "#Authors: Jinsung Yoon, James Jordon, Mihaela van der Schaar\n",
    "#https://github.com/jsyoon0823/GAIN\n",
    "\n",
    "#%% Packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# Arquivo data_loader.py\n",
    "#-------------------------------------------------------------------------\n",
    "# Necessary packages\n",
    "import numpy as np\n",
    "import numpy\n",
    "#from utils import binary_sampler\n",
    "from keras.datasets import mnist\n",
    "from python_utils import *\n",
    "import pandas as pd\n",
    "\n",
    "def binary_sampler(p, rows, cols):\n",
    "    unif_random_matrix = np.random.uniform(0.0, 1.0, size=[rows, cols])\n",
    "    binary_random_matrix = (unif_random_matrix < p).astype(np.float32)\n",
    "    return binary_random_matrix\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Arquivo utils.py\n",
    "#----------------------------------------------------------------------\n",
    "'''Utility functions for GAIN.\n",
    "(1) normalization: MinMax Normalizer\n",
    "(2) renormalization: Recover the data from normalzied data\n",
    "(3) rounding: Handlecategorical variables after imputation\n",
    "(4) rmse_loss: Evaluate imputed data in terms of RMSE\n",
    "(5) xavier_init: Xavier initialization\n",
    "(6) binary_sampler: sample binary random variables\n",
    "(7) uniform_sampler: sample uniform random variables\n",
    "(8) sample_batch_index: sample random batch index\n",
    "'''\n",
    " \n",
    "# Necessary packages\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "##IF USING TF 2 use following import to still use TF < 2.0 Functionalities\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "def normalization (data, parameters=None):\n",
    "  '''Normalize data in [0, 1] range.\n",
    "  \n",
    "  Args:\n",
    "    - data: original data\n",
    "  \n",
    "  Returns:\n",
    "    - norm_data: normalized data\n",
    "    - norm_parameters: min_val, max_val for each feature for renormalization\n",
    "  '''\n",
    "\n",
    "  # Parameters\n",
    "  _, dim = data.shape\n",
    "  norm_data = data.copy()\n",
    "  \n",
    "  if parameters is None:\n",
    "  \n",
    "    # MixMax normalization\n",
    "    min_val = np.zeros(dim)\n",
    "    max_val = np.zeros(dim)\n",
    "    \n",
    "    # For each dimension\n",
    "    for i in range(dim):\n",
    "      min_val[i] = np.nanmin(norm_data[:,i])\n",
    "      norm_data[:,i] = norm_data[:,i] - np.nanmin(norm_data[:,i])\n",
    "      max_val[i] = np.nanmax(norm_data[:,i])\n",
    "      norm_data[:,i] = norm_data[:,i] / (np.nanmax(norm_data[:,i]) + 1e-6)   \n",
    "      \n",
    "    # Return norm_parameters for renormalization\n",
    "    norm_parameters = {'min_val': min_val,\n",
    "                       'max_val': max_val}\n",
    "\n",
    "  else:\n",
    "    min_val = parameters['min_val']\n",
    "    max_val = parameters['max_val']\n",
    "    \n",
    "    # For each dimension\n",
    "    for i in range(dim):\n",
    "      norm_data[:,i] = norm_data[:,i] - min_val[i]\n",
    "      norm_data[:,i] = norm_data[:,i] / (max_val[i] + 1e-6)  \n",
    "      \n",
    "    norm_parameters = parameters    \n",
    "      \n",
    "  return norm_data, norm_parameters\n",
    "\n",
    "\n",
    "def renormalization (norm_data, norm_parameters):\n",
    "  '''Renormalize data from [0, 1] range to the original range.\n",
    "  \n",
    "  Args:\n",
    "    - norm_data: normalized data\n",
    "    - norm_parameters: min_val, max_val for each feature for renormalization\n",
    "  \n",
    "  Returns:\n",
    "    - renorm_data: renormalized original data\n",
    "  '''\n",
    "  \n",
    "  min_val = norm_parameters['min_val']\n",
    "  max_val = norm_parameters['max_val']\n",
    "\n",
    "  _, dim = norm_data.shape\n",
    "  renorm_data = norm_data.copy()\n",
    "    \n",
    "  for i in range(dim):\n",
    "    renorm_data[:,i] = renorm_data[:,i] * (max_val[i] + 1e-6)   \n",
    "    renorm_data[:,i] = renorm_data[:,i] + min_val[i]\n",
    "    \n",
    "  return renorm_data\n",
    "\n",
    "\n",
    "def rounding (imputed_data, data_x):\n",
    "  '''Round imputed data for categorical variables.\n",
    "  \n",
    "  Args:\n",
    "    - imputed_data: imputed data\n",
    "    - data_x: original data with missing values\n",
    "    \n",
    "  Returns:\n",
    "    - rounded_data: rounded imputed data\n",
    "  '''\n",
    "  \n",
    "  _, dim = data_x.shape\n",
    "  rounded_data = imputed_data.copy()\n",
    "  \n",
    "  for i in range(dim):\n",
    "    temp = data_x[~np.isnan(data_x[:, i]), i]\n",
    "    # Only for the categorical variable\n",
    "    if len(np.unique(temp)) < 20:\n",
    "      rounded_data[:, i] = np.round(rounded_data[:, i])\n",
    "      \n",
    "  return rounded_data\n",
    "\n",
    "\n",
    "def rmse_loss (ori_data, imputed_data, data_m):\n",
    "  '''Compute RMSE loss between ori_data and imputed_data\n",
    "  \n",
    "  Args:\n",
    "    - ori_data: original data without missing values\n",
    "    - imputed_data: imputed data\n",
    "    - data_m: indicator matrix for missingness\n",
    "    \n",
    "  Returns:\n",
    "    - rmse: Root Mean Squared Error\n",
    "  '''\n",
    "  \n",
    "  ori_data, norm_parameters = normalization(ori_data)\n",
    "  imputed_data, _ = normalization(imputed_data, norm_parameters)\n",
    "    \n",
    "  # Only for missing values\n",
    "  nominator = np.sum(((1-data_m) * ori_data - (1-data_m) * imputed_data)**2)\n",
    "  denominator = np.sum(1-data_m)\n",
    "  \n",
    "  rmse = np.sqrt(nominator/float(denominator))\n",
    "  \n",
    "  return rmse\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "  '''Xavier initialization.\n",
    "  \n",
    "  Args:\n",
    "    - size: vector size\n",
    "    \n",
    "  Returns:\n",
    "    - initialized random vector.\n",
    "  '''\n",
    "  in_dim = size[0]\n",
    "  xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "  return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "      \n",
    "\n",
    "def binary_sampler(p, rows, cols):\n",
    "  '''Sample binary random variables.\n",
    "  \n",
    "  Args:\n",
    "    - p: probability of 1\n",
    "    - rows: the number of rows\n",
    "    - cols: the number of columns\n",
    "    \n",
    "  Returns:\n",
    "    - binary_random_matrix: generated binary random matrix.\n",
    "  '''\n",
    "  unif_random_matrix = np.random.uniform(0., 1., size = [rows, cols])\n",
    "  binary_random_matrix = 1*(unif_random_matrix < p)\n",
    "  return binary_random_matrix\n",
    "\n",
    "\n",
    "def uniform_sampler(low, high, rows, cols):\n",
    "  '''Sample uniform random variables.\n",
    "  \n",
    "  Args:\n",
    "    - low: low limit\n",
    "    - high: high limit\n",
    "    - rows: the number of rows\n",
    "    - cols: the number of columns\n",
    "    \n",
    "  Returns:\n",
    "    - uniform_random_matrix: generated uniform random matrix.\n",
    "  '''\n",
    "  return np.random.uniform(low, high, size = [rows, cols])       \n",
    "\n",
    "\n",
    "def sample_batch_index(total, batch_size):\n",
    "  '''Sample index of the mini-batch.\n",
    "  \n",
    "  Args:\n",
    "    - total: total number of samples\n",
    "    - batch_size: batch size\n",
    "    \n",
    "  Returns:\n",
    "    - batch_idx: batch index\n",
    "  '''\n",
    "  total_idx = np.random.permutation(total)\n",
    "  batch_idx = total_idx[:batch_size]\n",
    "  return batch_idx\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Arquivo gain.py\n",
    "#---------------------------------------------------------------------\n",
    "'''GAIN function.\n",
    "Date: 2020/02/28\n",
    "Reference: J. Yoon, J. Jordon, M. van der Schaar, \"GAIN: Missing Data \n",
    "           Imputation using Generative Adversarial Nets,\" ICML, 2018.\n",
    "Paper Link: http://proceedings.mlr.press/v80/yoon18a/yoon18a.pdf\n",
    "Contact: jsyoon0823@gmail.com\n",
    "'''\n",
    "\n",
    "# Necessary packages\n",
    "#import tensorflow as tf\n",
    "##IF USING TF 2 use following import to still use TF < 2.0 Functionalities\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gain_imputation (data_x, gain_parameters):\n",
    "  '''Impute missing values in data_x\n",
    "  \n",
    "  Args:\n",
    "    - data_x: original data with missing values\n",
    "    - gain_parameters: GAIN network parameters:\n",
    "      - batch_size: Batch size\n",
    "      - hint_rate: Hint rate\n",
    "      - alpha: Hyperparameter\n",
    "      - iterations: Iterations\n",
    "      \n",
    "  Returns:\n",
    "    - imputed_data: imputed data\n",
    "  '''\n",
    "  # Define mask matrix\n",
    "  data_m = 1-np.isnan(data_x)\n",
    "  \n",
    "  # System parameters\n",
    "  batch_size = gain_parameters['batch_size']\n",
    "  hint_rate = gain_parameters['hint_rate']\n",
    "  alpha = gain_parameters['alpha']\n",
    "  iterations = gain_parameters['iterations']\n",
    "  \n",
    "  # Other parameters\n",
    "  no, dim = data_x.shape\n",
    "  \n",
    "  # Hidden state dimensions\n",
    "  h_dim = int(dim)\n",
    "  \n",
    "  # Normalization\n",
    "  norm_data, norm_parameters = normalization(data_x)\n",
    "  norm_data_x = np.nan_to_num(norm_data, 0)\n",
    "  \n",
    "  ## GAIN architecture   \n",
    "  # Input placeholders\n",
    "  # Data vector\n",
    "  X = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  # Mask vector \n",
    "  M = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  # Hint vector\n",
    "  H = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  \n",
    "  # Discriminator variables\n",
    "  D_W1 = tf.Variable(xavier_init([dim*2, h_dim])) # Data + Hint as inputs\n",
    "  D_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  D_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
    "  D_b2 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  D_W3 = tf.Variable(xavier_init([h_dim, dim]))\n",
    "  D_b3 = tf.Variable(tf.zeros(shape = [dim]))  # Multi-variate outputs\n",
    "  \n",
    "  theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "  \n",
    "  #Generator variables\n",
    "  # Data + Mask as inputs (Random noise is in missing components)\n",
    "  G_W1 = tf.Variable(xavier_init([dim*2, h_dim]))  \n",
    "  G_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  G_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
    "  G_b2 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  G_W3 = tf.Variable(xavier_init([h_dim, dim]))\n",
    "  G_b3 = tf.Variable(tf.zeros(shape = [dim]))\n",
    "  \n",
    "  theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
    "  \n",
    "  ## GAIN functions\n",
    "  # Generator\n",
    "  def generator(x,m):\n",
    "    # Concatenate Mask and Data\n",
    "    inputs = tf.concat(values = [x, m], axis = 1) \n",
    "    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)   \n",
    "    # MinMax normalized output\n",
    "    G_prob = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3) \n",
    "    return G_prob\n",
    "      \n",
    "  # Discriminator\n",
    "  def discriminator(x, h):\n",
    "    # Concatenate Data and Hint\n",
    "    inputs = tf.concat(values = [x, h], axis = 1) \n",
    "    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n",
    "    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n",
    "    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    return D_prob\n",
    "  \n",
    "  ## GAIN structure\n",
    "  # Generator\n",
    "  G_sample = generator(X, M)\n",
    " \n",
    "  # Combine with observed data\n",
    "  Hat_X = X * M + G_sample * (1-M)\n",
    "  \n",
    "  # Discriminator\n",
    "  D_prob = discriminator(Hat_X, H)\n",
    "  \n",
    "  ## GAIN loss\n",
    "  D_loss_temp = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) \\\n",
    "                                + (1-M) * tf.log(1. - D_prob + 1e-8)) \n",
    "  \n",
    "  G_loss_temp = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\n",
    "  \n",
    "  MSE_loss = \\\n",
    "  tf.reduce_mean((M * X - M * G_sample)**2) / tf.reduce_mean(M)\n",
    "  \n",
    "  D_loss = D_loss_temp\n",
    "  G_loss = G_loss_temp + alpha * MSE_loss \n",
    "  \n",
    "  ## GAIN solver\n",
    "  D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "  G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "  \n",
    "  ## Iterations\n",
    "  sess = tf.Session()\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "  # Start Iterations\n",
    "  for it in tqdm(range(iterations)):    \n",
    "      \n",
    "    # Sample batch\n",
    "    batch_idx = sample_batch_index(no, batch_size)\n",
    "    X_mb = norm_data_x[batch_idx, :]  \n",
    "    M_mb = data_m[batch_idx, :]  \n",
    "    # Sample random vectors  \n",
    "    Z_mb = uniform_sampler(0, 0.01, batch_size, dim) \n",
    "    # Sample hint vectors\n",
    "    H_mb_temp = binary_sampler(hint_rate, batch_size, dim)\n",
    "    H_mb = M_mb * H_mb_temp\n",
    "      \n",
    "    # Combine random vectors with observed vectors\n",
    "    X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
    "      \n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss_temp], \n",
    "                              feed_dict = {M: M_mb, X: X_mb, H: H_mb})\n",
    "    _, G_loss_curr, MSE_loss_curr = \\\n",
    "    sess.run([G_solver, G_loss_temp, MSE_loss],\n",
    "             feed_dict = {X: X_mb, M: M_mb, H: H_mb})\n",
    "            \n",
    "  ## Return imputed data      \n",
    "  Z_mb = uniform_sampler(0, 0.01, no, dim) \n",
    "  M_mb = data_m\n",
    "  X_mb = norm_data_x          \n",
    "  X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
    "      \n",
    "  imputed_data = sess.run([G_sample], feed_dict = {X: X_mb, M: M_mb})[0]\n",
    "  \n",
    "  imputed_data = data_m * norm_data_x + (1-data_m) * imputed_data\n",
    "  \n",
    "  # Renormalization\n",
    "  imputed_data = renormalization(imputed_data, norm_parameters)  \n",
    "  \n",
    "  # Rounding\n",
    "  imputed_data = rounding(imputed_data, data_x)  \n",
    "\n",
    "  #a = numpy.asarray(imputed_data)\n",
    "  #numpy.savetxt(\"imputed_data.csv\", a, delimiter=\",\") \n",
    "          \n",
    "  return imputed_data\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "# Arquivo main.py\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# Necessary packages\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "#from data_loader import data_loader\n",
    "#from gain import gain\n",
    "#from utils import rmse_loss\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcimpute.gaussian_copula import GaussianCopula\n",
    "\n",
    "def copula_imputation(missing_data):\n",
    "    \n",
    "    columns = missing_data.columns\n",
    "    \n",
    "    # model fitting \n",
    "    model = GaussianCopula(verbose=1)\n",
    "    \n",
    "    # Impute the missing data\n",
    "    complete_data = model.fit_transform(missing_data)   \n",
    "    \n",
    "    # Get the dataframe\n",
    "    complete_data = pd.DataFrame(complete_data, columns = columns)\n",
    "    \n",
    "    return complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rpy2\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.robjects import pandas2ri\n",
    "import rpy2.robjects as ro\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")   \n",
    "\n",
    "def _none2null(none_obj):\n",
    "    return ro.r(\"NULL\")\n",
    "    \n",
    "def hotdeck_imputation(missing_data):\n",
    "    r_vim = importr('VIM')\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "      r_from_pd_df = ro.conversion.py2rpy(missing_data)\n",
    "\n",
    "\n",
    "    result = r_vim.hotdeck(r_from_pd_df) #Run hotdeck imputation\n",
    "    #complete_data = pd.DataFrame(result, axis = 1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
